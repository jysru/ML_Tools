{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data to build a dataset from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "tf.Tensor([0.7196431 0.7576424], shape=(2,), dtype=float32)\n",
      "tf.Tensor([0.5566189 0.9885305], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import mltools.dataset as dtools\n",
    "\n",
    "# Create a simple unbatched dataset\n",
    "# unbatched_dataset = tf.data.Dataset.from_tensor_slices((tf.random.uniform([100, 2]), tf.random.uniform([100, 2])))\n",
    "unbatched_dataset = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 2]))\n",
    "\n",
    "# Create a batched dataset\n",
    "batched_dataset = unbatched_dataset.batch(16)\n",
    "\n",
    "print(dtools.tf_is_dataset_batched(unbatched_dataset))\n",
    "print(dtools.tf_is_dataset_batched(batched_dataset))\n",
    "\n",
    "print(dtools.tf_get_random_element_from_dataset(unbatched_dataset))\n",
    "print(dtools.tf_get_random_element_from_dataset(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  96\n",
      "Validation size:  18\n",
      "Test size:  6\n"
     ]
    }
   ],
   "source": [
    "import mltools.dataset as dtools\n",
    "\n",
    "# Example inputs and labels\n",
    "length = 120\n",
    "inputs = np.random.rand(length, 2, 2, 1)  # 100 images of size 32x32x3\n",
    "labels = np.random.rand(length, 4)  # 100 labels (for a 10-class classification)\n",
    "\n",
    "splitter_func = dtools.tf_make_datasets_from_tensor_slices\n",
    "# splitter_func = dtools.tf_make_datasets_from_sklearn_arrays\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = splitter_func(\n",
    "    inputs=inputs,\n",
    "    outputs=labels,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.15,\n",
    "    seed=42,\n",
    "    batch_size=16,\n",
    "    avoid_leakage=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  96\n",
      "Validation size:  18\n",
      "Test size:  6\n",
      "(<tf.Tensor: shape=(2, 2, 1), dtype=float64, numpy=\n",
      "array([[[0.63239132],\n",
      "        [0.15647672]],\n",
      "\n",
      "       [[0.47392122],\n",
      "        [0.71639711]]])>, <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.80571453, 0.32817948, 0.20905937, 0.08468834])>)\n"
     ]
    }
   ],
   "source": [
    "import mltools.dataset as dtools\n",
    "\n",
    "# Example inputs and labels\n",
    "length = 120\n",
    "inputs = np.random.rand(length, 2, 2, 1)  # 100 images of size 32x32x3\n",
    "labels = np.random.rand(length, 4)  # 100 labels (for a 10-class classification)\n",
    "\n",
    "splitter_func = dtools.tf_make_datasets_from_tensor_slices\n",
    "# splitter_func = dtools.tf_make_datasets_from_sklearn_arrays\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = splitter_func(\n",
    "    inputs=inputs,\n",
    "    outputs=labels,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.15,\n",
    "    seed=42,\n",
    "    batch_size=None,\n",
    "    avoid_leakage=False,\n",
    ")\n",
    "\n",
    "print(\n",
    "    dtools.tf_get_random_element_from_dataset(train_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data leakage detected between train and val: 17 common elements found.\n",
      "Data leakage detected between train and test: 5 common elements found.\n",
      "Data leakage detected between val and test: 1 common elements found.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dtools.tf_are_datasets_leaking(\n",
    "    dict(\n",
    "        train = train_dataset,\n",
    "        val = val_dataset,\n",
    "        test = test_dataset,\n",
    "    )\n",
    ")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
