{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data to build a dataset from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.RandomDataset object at 0x0000017E5429BD60>\n"
     ]
    }
   ],
   "source": [
    "import mltools.dataset as dtools\n",
    "\n",
    "class RandomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, n: int, m: int, N: int, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self._n = n\n",
    "        self._m = m\n",
    "        self._N = N\n",
    "        self.device = device\n",
    "        self.__make()\n",
    "        \n",
    "    def __make(self):\n",
    "        self._X = torch.rand([self._N, self._n], device=self.device)\n",
    "        self._Y = torch.rand([self._N, self._m], device=self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._N\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self._X[idx, :], self._Y[idx, :]       \n",
    "\n",
    "\n",
    "unbatched_dset = RandomDataset(5, 2, 1000)\n",
    "print(unbatched_dset)\n",
    "\n",
    "\n",
    "# # unbatched_dataset = tf.data.Dataset.from_tensor_slices((tf.random.uniform([100, 2]), tf.random.uniform([100, 2])))\n",
    "# unbatched_dataset = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 2]))\n",
    "\n",
    "# # Create a batched dataset\n",
    "# batched_dataset = unbatched_dataset.batch(16)\n",
    "\n",
    "# print(dtools.tf_is_dataset_batched(unbatched_dataset))\n",
    "# print(dtools.tf_is_dataset_batched(batched_dataset))\n",
    "\n",
    "# print(dtools.tf_get_random_element_from_dataset(unbatched_dataset))\n",
    "# print(dtools.tf_get_random_element_from_dataset(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "train_set, val_set = torch.utils.data.random_split(unbatched_dset, [0.8, 0.2], generator=generator)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(dtools.torch_is_dataloader_batched(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1667, 0.6292, 0.4831, 0.2742, 0.7753]), tensor([0.7983, 0.7667]))\n"
     ]
    }
   ],
   "source": [
    "elem = dtools.torch_get_random_element_from_dataloader(val_loader)\n",
    "print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  96\n",
      "Validation size:  18\n",
      "Test size:  6\n"
     ]
    }
   ],
   "source": [
    "import mltools.dataset as dtools\n",
    "\n",
    "# Example inputs and labels\n",
    "length = 120\n",
    "inputs = np.random.rand(length, 2, 2, 1)  # 100 images of size 32x32x3\n",
    "labels = np.random.rand(length, 4)  # 100 labels (for a 10-class classification)\n",
    "\n",
    "splitter_func = dtools.tf_make_datasets_from_tensor_slices\n",
    "# splitter_func = dtools.tf_make_datasets_from_sklearn_arrays\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = splitter_func(\n",
    "    inputs=inputs,\n",
    "    outputs=labels,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.15,\n",
    "    seed=42,\n",
    "    batch_size=16,\n",
    "    avoid_leakage=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  96\n",
      "Validation size:  16\n",
      "Test size:  8\n",
      "(<tf.Tensor: shape=(2, 2, 1), dtype=float64, numpy=\n",
      "array([[[0.95652186],\n",
      "        [0.48453802]],\n",
      "\n",
      "       [[0.59427153],\n",
      "        [0.06836859]]])>, <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.28196447, 0.10134534, 0.30883358, 0.37356605])>)\n"
     ]
    }
   ],
   "source": [
    "import mltools.dataset as dtools\n",
    "\n",
    "# Example inputs and labels\n",
    "length = 120\n",
    "inputs = np.random.rand(length, 2, 2, 1)  # 100 images of size 32x32x3\n",
    "labels = np.random.rand(length, 4)  # 100 labels (for a 10-class classification)\n",
    "\n",
    "# splitter_func = dtools.tf_make_datasets_from_tensor_slices\n",
    "splitter_func = dtools.tf_make_datasets_from_sklearn_arrays\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = splitter_func(\n",
    "    inputs=inputs,\n",
    "    outputs=labels,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.15,\n",
    "    seed=42,\n",
    "    batch_size=None,\n",
    "    # avoid_leakage=False,\n",
    ")\n",
    "\n",
    "print(\n",
    "    dtools.tf_get_random_element_from_dataset(train_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data leakage detected between train and val.\n",
      "No data leakage detected between train and test.\n",
      "No data leakage detected between val and test.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dtools.tf_are_datasets_leaking(\n",
    "    dict(\n",
    "        train = train_dataset,\n",
    "        val = val_dataset,\n",
    "        test = test_dataset,\n",
    "    )\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mdtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_are_datasets_leaking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\users\\saucourt\\coding\\ml_tools\\mltools\\dataset.py:354\u001b[0m, in \u001b[0;36mtorch_are_datasets_leaking\u001b[1;34m(dataloaders)\u001b[0m\n\u001b[0;32m    351\u001b[0m dataloaders_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dataloaders\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, name1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloaders_names):\n\u001b[1;32m--> 354\u001b[0m     set1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_dataloader_to_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name2 \u001b[38;5;129;01min\u001b[39;00m dataloaders_names[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    357\u001b[0m         set2 \u001b[38;5;241m=\u001b[39m torch_dataloader_to_set(dataloaders[name2])\n",
      "File \u001b[1;32mc:\\users\\saucourt\\coding\\ml_tools\\mltools\\dataset.py:318\u001b[0m, in \u001b[0;36mtorch_dataloader_to_set\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtorch_dataloader_to_set\u001b[39m(dataloader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader,) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mset\u001b[39m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch_is_dataloader_batched(dataloader):\n\u001b[1;32m--> 318\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_batched_dataloader_to_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch_unbatched_dataloader_to_set(dataloader)\n",
      "File \u001b[1;32mc:\\users\\saucourt\\coding\\ml_tools\\mltools\\dataset.py:331\u001b[0m, in \u001b[0;36mtorch_batched_dataloader_to_set\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# Flatten the batch and add elements to the set\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     all_elements\u001b[38;5;241m.\u001b[39mupdate(inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m--> 331\u001b[0m     \u001b[43mall_elements\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_elements\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dtools.torch_are_datasets_leaking(\n",
    "    dict(\n",
    "        train = train_loader,\n",
    "        val = val_loader,\n",
    "    )\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data leakage detected. Train and validation sets are disjoint.\n",
      "Percentage of overlapping elements: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def get_all_elements(dataloader):\n",
    "    all_elements = set()\n",
    "    for batch in dataloader:\n",
    "        # Assuming each batch is a tuple of (inputs, labels)\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        # Convert tensors to tuples for hashability\n",
    "        inputs_tuple = tuple(tuple(input.cpu().numpy().flatten().tolist()) for input in inputs)\n",
    "        labels_tuple = tuple(tuple(label.cpu().numpy().flatten().tolist()) for label in labels)\n",
    "        \n",
    "        # Add to set\n",
    "        all_elements.update(zip(inputs_tuple, labels_tuple))\n",
    "    \n",
    "    return all_elements\n",
    "\n",
    "# Extract elements from train and val loaders\n",
    "train_elements = get_all_elements(train_loader)\n",
    "val_elements = get_all_elements(val_loader)\n",
    "\n",
    "# Check for intersection\n",
    "intersection = train_elements.intersection(val_elements)\n",
    "\n",
    "if len(intersection) > 0:\n",
    "    print(f\"Warning: Data leakage detected! {len(intersection)} elements are present in both train and validation sets.\")\n",
    "    print(\"Sample of overlapping elements:\")\n",
    "    for elem in list(intersection)[:5]:  # Print first 5 overlapping elements\n",
    "        print(elem)\n",
    "else:\n",
    "    print(\"No data leakage detected. Train and validation sets are disjoint.\")\n",
    "\n",
    "# Calculate percentage of overlap\n",
    "total_elements = len(train_elements) + len(val_elements)\n",
    "overlap_percentage = len(intersection) / total_elements * 100\n",
    "\n",
    "print(f\"Percentage of overlapping elements: {overlap_percentage:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
